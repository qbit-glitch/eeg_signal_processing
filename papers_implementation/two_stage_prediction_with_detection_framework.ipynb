{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beacb0f9",
   "metadata": {},
   "source": [
    "# A Two-Stage Prediction + Detection Framework for Real-time Epileptic Seizure Monitoring\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281714b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Two Stage Prediction + Detection framework: for Real time Epileptic Seizure Prediction\n"
     ]
    }
   ],
   "source": [
    "print(\"A Two Stage Prediction + Detection framework: for Real time Epileptic Seizure Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06087dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp310-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/qbit-glitch/Desktop/coding-projects/eeg_signal_processing/venv_2/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/qbit-glitch/Desktop/coding-projects/eeg_signal_processing/venv_2/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/qbit-glitch/Desktop/coding-projects/eeg_signal_processing/venv_2/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading torch-2.9.1-cp310-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m390.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [torch]32m5/6\u001b[0m [torch]kx]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.20.0 fsspec-2025.10.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e86a9",
   "metadata": {},
   "source": [
    "## Workflow and Architecture of the propose two-stage prediction + detection model : PDNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2723359",
   "metadata": {},
   "source": [
    "![PDNet Architecure](./assets/PDNet_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356fc22",
   "metadata": {},
   "source": [
    "![Structure Parameters of the Proposed PDNet Model](./assets/parameters_of_PDNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee731116",
   "metadata": {},
   "source": [
    "Code for ResConv is adapted from : [Convolution with Residual Connection](https://medium.com/@chen-yu/building-a-customized-residual-cnn-with-pytorch-471810e894ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ddc6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef47ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, mid_ch:int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = in_ch,\n",
    "                               out_channels = mid_ch,\n",
    "                               kernel_size = 1,\n",
    "                               padding = 0)\n",
    "        # self.bn1 = nn.BatchNorm1d(mid_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels = mid_ch,\n",
    "                               out_channels = out_ch,\n",
    "                               kernel_size = 5,\n",
    "                               padding = 2)\n",
    "        # self.bn2 = nn.BatchNorm1d(out_ch)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor :\n",
    "        x = self.relu(self.conv1(x))\n",
    "        # print(x.shape)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "    \n",
    "conv_block = ConvBlock(8,2,16)\n",
    "x = torch.randn((16,8,256))\n",
    "conv_block(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38126cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8, 512])\n",
      "torch.Size([16, 8, 256])\n",
      "torch.Size([16, 16, 256])\n",
      "torch.Size([16, 16, 128])\n",
      "torch.Size([16, 16, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SharedLayer(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_ch, out_channels=out_ch, kernel_size=5, padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = ConvBlock(8,2,16)\n",
    "        self.conv3 = ConvBlock(16,4,16)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        print(x.shape)\n",
    "        x = self.maxpool(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        print(x.shape)\n",
    "        x = self.maxpool(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "        \n",
    "x = torch.randn((16,512)).unsqueeze(dim=1)\n",
    "shared_layer = SharedLayer(1,8)\n",
    "shared_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebeac89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((16,8,256))\n",
    "conv_block = ConvBlock(8,2,16)\n",
    "conv_block(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e49840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((16,512)).unsqueeze(1)\n",
    "conv = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5, padding=2)\n",
    "conv(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44b71e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ec62f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 128])\n",
      "torch.Size([1, 16, 128])\n",
      "torch.Size([1, 16, 128])\n",
      "torch.Size([1, 32, 128])\n",
      "torch.Size([1, 32, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResConv(nn.Module):\n",
    "    def __init__(self, in_ch1:int, in_ch2:int, in_ch3: int, out_ch: int) -> None :\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_ch1,\n",
    "                               out_channels = in_ch2,\n",
    "                               kernel_size = 1,\n",
    "                               padding = 0)\n",
    "        self.bn1 = nn.BatchNorm1d(in_ch2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels = in_ch2,\n",
    "                               out_channels = in_ch3,\n",
    "                               kernel_size = 5,\n",
    "                               padding = 2)\n",
    "        self.bn2 = nn.BatchNorm1d(in_ch3)\n",
    "        self.conv3 = nn.Conv1d(in_channels=in_ch3,\n",
    "                               out_channels = out_ch,\n",
    "                               kernel_size = 1,\n",
    "                               padding = 0)\n",
    "        self.bn3 = nn.BatchNorm1d(out_ch)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        print(x.shape)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        print(x.shape)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        print(x.shape)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "res_conv = ResConv(256,16,16,32)\n",
    "\n",
    "x = torch.randn((256,128)).unsqueeze(dim=0)\n",
    "res_conv(x).squeeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ccc8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing for flattening the tensor to specified size as per the paper\n",
    "x = torch.randn((16,16,128))\n",
    "flatten = nn.Flatten(start_dim=0, end_dim=1)\n",
    "flatten(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc8f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462fc8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the global average pooling\n",
    "class GlobalAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.mean(dim=2)\n",
    "    \n",
    "    \n",
    "x = torch.randn((1,64,128))\n",
    "gap = GlobalAveragePooling()\n",
    "gap(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b9f730",
   "metadata": {},
   "source": [
    "**How to Calculate Padding :** \n",
    "\n",
    "L_out = (L_in + 2 * padding - kernel_size) / stride + 1\n",
    "\n",
    "Since stride = 1:\n",
    "\n",
    "=> L_out = L_in + 2 * padding - kernel_size + 1   (L_in = L_out)\n",
    "\n",
    "=> padding = (kernel_size - 1)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c633f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDNetModelV1(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, is_shared_layer = False):\n",
    "        super().__init__()\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            SharedLayer(in_ch = in_ch, out_ch=out_ch)\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=1)\n",
    "        \n",
    "        self.prediction_layer = nn.Sequential(\n",
    "            ConvBlock(256,8,64),\n",
    "            GlobalAveragePooling(),\n",
    "            nn.Linear(in_features=64, out_features=2)\n",
    "        )\n",
    "        \n",
    "        self.detection_layer = nn.Sequential(\n",
    "            ResConv(256,16,16,32),\n",
    "            ResConv(32,32,32,48),\n",
    "            ResConv(48,48,48,64),   \n",
    "            GlobalAveragePooling(),\n",
    "            nn.Linear(in_features=64, out_features=11)\n",
    "        )\n",
    "        \n",
    "        self.is_shared_layer = is_shared_layer\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.shared_layer(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        if self.is_shared_layer == False :\n",
    "            x = x.unsqueeze(dim=0)\n",
    "            x = self.prediction_layer(x).squeeze(0)\n",
    "            return x\n",
    "        \n",
    "        # x1 = self.prediction_layer(x).squeeze(dim=0)\n",
    "        # print(x1.shape)\n",
    "        \n",
    "        x = x.unsqueeze(dim=0)\n",
    "        x = self.detection_layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1679a36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 512])\n",
      "torch.Size([16, 8, 512])\n",
      "torch.Size([16, 8, 256])\n",
      "torch.Size([16, 16, 256])\n",
      "torch.Size([16, 16, 128])\n",
      "torch.Size([16, 16, 128])\n",
      "torch.Size([16, 16, 128])\n",
      "torch.Size([256, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_0 = PDNetModelV1(in_ch=1, out_ch=8, is_shared_layer=False)\n",
    "\n",
    "x = torch.randn((16,512)).unsqueeze(dim=1)\n",
    "print(x.shape)\n",
    "model_1_0(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53eaaef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 512])\n",
      "torch.Size([16, 8, 512])\n",
      "torch.Size([16, 8, 256])\n",
      "torch.Size([16, 16, 256])\n",
      "torch.Size([16, 16, 128])\n",
      "torch.Size([16, 16, 128])\n",
      "torch.Size([16, 16, 128])\n",
      "torch.Size([256, 128])\n",
      "torch.Size([1, 256, 128])\n",
      "torch.Size([1, 16, 128])\n",
      "torch.Size([1, 16, 128])\n",
      "torch.Size([1, 32, 128])\n",
      "torch.Size([1, 32, 128])\n",
      "torch.Size([1, 32, 128])\n",
      "torch.Size([1, 32, 128])\n",
      "torch.Size([1, 32, 128])\n",
      "torch.Size([1, 48, 128])\n",
      "torch.Size([1, 48, 128])\n",
      "torch.Size([1, 48, 128])\n",
      "torch.Size([1, 48, 128])\n",
      "torch.Size([1, 48, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "torch.Size([1, 64, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_1 = PDNetModelV1(in_ch=1, out_ch=8, is_shared_layer=True)\n",
    "\n",
    "x = torch.randn((16,512)).unsqueeze(dim=1)\n",
    "print(x.shape)\n",
    "model_1_1(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c9699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
